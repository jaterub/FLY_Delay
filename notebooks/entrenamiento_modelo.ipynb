{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lectura df procesado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv('../data/processed/processed.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Division Train y test a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_df, test_df = train_test_split(df_processed, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def guardar_en_csv(df, ruta_destino):\n",
    "   \n",
    "    df.to_csv(ruta_destino, index=False)\n",
    "    print(f\"DataFrame guardado en {ruta_destino}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado en ../data/train/train_df.csv\n"
     ]
    }
   ],
   "source": [
    "guardar_en_csv(train_df,'../data/train/train_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame guardado en ../data/test/test_df.csv\n"
     ]
    }
   ],
   "source": [
    "guardar_en_csv(test_df,'../data/test/test_df.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_processed.drop('DELAYED', axis=1).copy()\n",
    "\n",
    "y = df_processed['DELAYED'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DELAYED\n",
       "0    37433\n",
       "1    20758\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_processed['DELAYED'].value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic Regression (Regresión Logística)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.27828851275883\n",
      "[[7418  112]\n",
      " [1879 2230]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " NOT DELAYED       0.80      0.99      0.88      7530\n",
      "     DELAYED       0.95      0.54      0.69      4109\n",
      "\n",
      "    accuracy                           0.83     11639\n",
      "   macro avg       0.88      0.76      0.79     11639\n",
      "weighted avg       0.85      0.83      0.81     11639\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\34680\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:432: UserWarning: X has feature names, but LogisticRegression was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "accuracy = lr.score(X_test_scaled, y_test)\n",
    "print(\"Accuracy:\", accuracy *100)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "clr = classification_report(y_test, y_pred, target_names=[\"NOT DELAYED\", \"DELAYED\"])\n",
    "\n",
    "print(cm)\n",
    "print(clr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mejora de LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  \n",
    "    ('lr', LogisticRegression(max_iter=1000))  \n",
    "])\n",
    "\n",
    "# Hiperparámetros \n",
    "parameters = {\n",
    "    'lr__C': [0.1, 1, 0.01], \n",
    "    'lr__penalty': ['l2']  \n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall', verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas Logistic Regression\n",
      "accuracy: 99.28%\n",
      "precision: 99.31%\n",
      "recall: 98.64%\n",
      "F1 score: 98.97%\n",
      "Matriz de confusión:\n",
      "[[7502   28]\n",
      " [  56 4053]]\n"
     ]
    }
   ],
   "source": [
    "print('Metricas Logistic Regression')\n",
    "\n",
    "# Predecir best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\n",
    "# Calcula métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "confusion = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "print(\"precision: {:.2f}%\".format(precision * 100))\n",
    "print(\"recall: {:.2f}%\".format(recall * 100))\n",
    "print(\"F1 score: {:.2f}%\".format(f1 * 100))\n",
    "\n",
    "print(\"Matriz de confusión:\")\n",
    "print(confusion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Naive Bayes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;nb&#x27;, GaussianNB(var_smoothing=1e-07))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n",
       "                (&#x27;nb&#x27;, GaussianNB(var_smoothing=1e-07))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB(var_smoothing=1e-07)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('nb', GaussianNB(var_smoothing=1e-07))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalador\n",
    "    ('nb', GaussianNB())  # Clasificador Naive Bayes\n",
    "])\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "parameters = {\n",
    "    'nb__var_smoothing': [1e-9, 1e-8, 1e-7]  \n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas Naive Bias\n",
      "accuracy: 38.73%\n",
      "Precisión: 36.25%\n",
      "recall: 96.91%\n",
      "f1_score : 52.76%\n"
     ]
    }
   ],
   "source": [
    "print('Metricas Naive Bias')\n",
    "# Predecir con el mejor modelo en los datos de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precisión: {:.2f}%\".format(precision * 100))\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"recall: {:.2f}%\".format(recall * 100))\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"f1_score : {:.2f}%\".format(f1 * 100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metricas Random Forest\n",
      "Accuracy: 100.00%\n",
      "Precisión: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalador\n",
    "    ('rf', RandomForestClassifier())  # Clasificador Random Forest\n",
    "])\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "parameters = {\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [ 5, 10]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print('Metricas Random Forest')\n",
    "\n",
    "# Predecir con el mejor modelo en los datos de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precisión: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas Random Forest\n",
    "Accuracy: 100.00%\n",
    "Precisión: 100.00%\n",
    "Recall: 100.00%\n",
    "F1-score: 100.00%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Gradient Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalador\n",
    "    ('gb', GradientBoostingClassifier())  # Clasificador Gradient Boosting\n",
    "])\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "parameters = {\n",
    "    'gb__n_estimators': [100, 200, 300],\n",
    "    'gb__learning_rate': [0.1, 0.05, 0.01],\n",
    "    'gb__max_depth': [3, 1, 2]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Métricas Gradient Boosting\n",
      "Accuracy: 100.00%\n",
      "Precisión: 100.00%\n",
      "Recall: 100.00%\n",
      "F1-score: 100.00%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print('Métricas Gradient Boosting')\n",
    "\n",
    "# Predecir con el mejor modelo en los datos de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "print(\"Precisión: {:.2f}%\".format(precision * 100))\n",
    "\n",
    "recall = recall_score(y_test, y_pred)\n",
    "print(\"Recall: {:.2f}%\".format(recall * 100))\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(\"F1-score: {:.2f}%\".format(f1 * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar las librerías necesarias\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, learning_curve\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definir el pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Escalador\n",
    "    ('gb', GradientBoostingClassifier())  # Clasificador Gradient Boosting\n",
    "])\n",
    "\n",
    "# Definir los hiperparámetros a ajustar\n",
    "parameters = {\n",
    "    'gb__n_estimators': [100, 200, 300],\n",
    "    'gb__learning_rate': [0.1, 0.001, 0.01],\n",
    "    'gb__max_depth': [3, 1, 2],\n",
    "    'gb__alpha': [0.1, 0.5, 0.9] # Añadir el parámetro alpha para la regularización\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando validación cruzada\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5, scoring='recall')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtener las métricas del mejor modelo en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(f\"Métricas del mejor modelo:\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Precisión: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"F1-score: {f1:.2%}\")\n",
    "\n",
    "# Obtener las curvas de aprendizaje del mejor modelo\n",
    "train_sizes, train_scores, test_scores = learning_curve(best_model, X_train, y_train, cv=5)\n",
    "\n",
    "# Calcular las medias y desviaciones estándar de las puntuaciones de entrenamiento y prueba\n",
    "train_scores_mean = train_scores.mean(axis=1)\n",
    "train_scores_std = train_scores.std(axis=1)\n",
    "test_scores_mean = test_scores.mean(axis=1)\n",
    "test_scores_std = test_scores.std(axis=1)\n",
    "\n",
    "# Graficar las curvas de aprendizaje\n",
    "plt.figure()\n",
    "plt.title(\"Curvas de aprendizaje\")\n",
    "plt.xlabel(\"Tamaño del conjunto de entrenamiento\")\n",
    "plt.ylabel(\"Puntuación\")\n",
    "plt.grid()\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                 train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                 color=\"r\")\n",
    "plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                 test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                 color=\"g\")\n",
    "plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "         label=\"Puntuación de entrenamiento\")\n",
    "plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "         label=\"Puntuación de prueba\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el modelo entrenado en un archivo pickle\n",
    "with open('../models/trained_model.pkl', 'wb') as archivo_salida:\n",
    "    pickle.dump(best_model, archivo_salida)\n",
    "\n",
    "# Guardar la configuración del modelo en un archivo YAML\n",
    "model_config = {\n",
    "    'model_name': 'Naive Bayes',\n",
    "    'best_params': best_model.get_params()\n",
    "}\n",
    "\n",
    "with open('../models/model_config.yaml', 'w') as f:\n",
    "    yaml.dump(model_config, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
